---
title: "Parte 1: Arquitectura comparativa de Nested Virtualization en Hyper-V, KVM y VMware"
description: ""
date: "May 23, 2025"
---

¬øPor qu√© Docker Desktop s√≠ puede coexistir con Hyper-V y WSL2, pero VMware no?

Estaba intentando mostar un cluster de alta disponibilidad usando vmware, ya que irme a la r√°pida y usar una implementaci√≥n de Kind (kubernetes in docker) no tendr√≠a un entorno real para probar t√©cnicas de arquitectura de alta disponibilidad o sharding por ejemplo. Resulta que como buen desarrollador de windows, tenia habilitado docker y hyper-v, pero al momento de instalar proxmox ve (un sistema operativo en el cual puedo provisionar varias vm dentro de √©l) ten√≠a un mensaje de que mi sistema no soporta KVM, algo esencial para hacer virtualizaci√≥n anidada. Pero ¬øqu√© es KVM? para eso primero tenemos que remontarnos a los hipervisores. 

# Hipervisores

> Un hipervisor es un software que se puede utilizar para ejecutar varias m√°quinas virtuales en una √∫nica m√°quina f√≠sica. [1](https://aws.amazon.com/es/what-is/hypervisor/#:~:text=Un%20hipervisor%20es%20un%20software,operativo%20y%20de%20aplicaciones%20propios.) 

Existen de dos tipos:
- Tipo 1 (bare-metal): corre directo sobre el hardware (ej: ESXi, KVM, Hyper-V Server). 
- Tipo 2 (hosted): corre como una app sobre un OS (ej: VirtualBox, VMware Workstation). 
Su funci√≥n principal: virtualizar el hardware para que m√∫ltiples sistemas operativos puedan usarlo de forma aislada y compartida. 

Exactamente en ese momento con vmware estaba usando un hipervisor de tipo 2.  Preguntarnos por el origen de los hipervisores nos remontar√≠a a los a√±os 60' y la verdad, no viene al caso su funcionamiento ni arquitectura. Basta con saber algunos detalles.

## 1. Primera generaci√≥n (1960s‚Äì2000): Virtualizaci√≥n base

- **Objetivo:** que *una sola m√°quina f√≠sica* corra m√∫ltiples sistemas operativos simult√°neamente.
- **Ejemplo:** IBM CP-40 (1967), VM/370 ‚Üí corr√≠an m√∫ltiples CMS sobre un solo mainframe.
- **Hipervisores tipo 1 nacen aqu√≠**: controlan el hardware directamente.
- No se necesitaba nested virtualization porque el objetivo era **compartir el hardware**, no anidar mundos virtuales.

Entonces, estamos bien hasta este punto. Los hipervisores de tipo 1 son buenos, ya que:
- Los mainframes ya daban aislamiento fuerte.
- Nadie necesitaba correr un hipervisor *dentro* de una VM.
- Las VMs ya cumpl√≠an el objetivo de compartir recursos.

Pero surge la necesidad de correr un hipervisor dentro de un un so como nuestro querido virtualbox o vmware, asi que comienza la era del hipervisor de tipo 2. 

## 2. **Virtualizaci√≥n x86 (2000s)**

- **VMware** populariza la virtualizaci√≥n comercial sobre x86 sin soporte nativo del hardware.
- Luego aparecen Intel VT-x y AMD-V (~2005), que lo hacen eficiente.
- Empiezan los clouds (EC2 en 2006), y ahora s√≠: **hay razones para virtualizar dentro de m√°quinas virtualizadas**:
    - Testear tu propio cloud dentro de AWS
    - Correr Hyper-V dentro de una VM
    - Tener un laboratorio de clusters en un datacenter sin m√°quinas f√≠sicas adicionales

De hecho, si te das cuenta en este punto aparecieron los famosos clouds, como la maquina virtual de amazon ec2. Pero realmente esto nos ofrece una maquina virtual dentro del so y no tenemos un problema real. El tema aqu√≠ es cuando queremos crear una m√°quina virtual, dentro de otra. 

## 3. **Turtles (2010)**

- Hasta ese momento, **no exist√≠a una soluci√≥n t√©cnica real para permitir anidar virtualizaci√≥n** en x86.
- Turtles implementa nested virtualization en KVM, **antes de que el hardware lo soporte completamente**.
- Fue un **salto t√©cnico** que abri√≥ la posibilidad de tener *infraestructura dentro de la infraestructura*.

## 5. **NestCloud (2011) lo hace viable a escala cloud**

- Toma lo que hizo Turtles, pero le aplica mejoras usando las nuevas capacidades de **Intel VT-x con EPT**.
- Hace que la anidaci√≥n tenga **bajo overhead** y pueda usarse en producci√≥n en entornos como OpenStack o Azure.




Aunque [NestCloud](/static/nestcloud-towards-practical-nested-virtualization.pdf) fue una de las primeras arquitecturas pr√°cticas de virtualizaci√≥n anidada con bajo overhead, no fue la primera en implementar esta tecnolog√≠a. Por ejemplo, el proyecto Turtles present√≥ en 2010 una soluci√≥n de virtualizaci√≥n anidada en el hipervisor KVM, logrando un rendimiento cercano al de la virtualizaci√≥n de un solo nivel.

##  ¬øQu√© aport√≥ NestCloud?
Antes de NestCloud, las soluciones de virtualizaci√≥n anidada en plataformas x86 se basaban principalmente en emulaci√≥n, lo que resultaba en un rendimiento deficiente y una usabilidad limitada. NestCloud introdujo tres optimizaciones clave para reducir la sobrecarga de las m√°quinas virtuales anidadas:


1. Guest Page Fault Bypassing: Permit√≠a que las m√°quinas virtuales anidadas manejaran fallos de p√°gina sin necesidad de una salida de m√°quina virtual (VM Exit).
2. Virtual EPT (Extended Page Table): Eliminaba fallos de p√°gina innecesarios introducidos por las tablas de p√°ginas sombra en el monitor de m√°quina virtual anidado.
3. PV VMCS (Paravirtualized Virtual Machine Control Structure): Proporcionaba un acceso m√°s eficiente a la estructura de control de la m√°quina virtual para el monitor de m√°quina virtual anidado.

Los resultados experimentales mostraron que el rendimiento de las m√°quinas virtuales anidadas en NestCloud era cercano al de las m√°quinas virtuales de un solo nivel en pruebas intensivas de CPU y memoria, con una sobrecarga de CPU del 5.22% y una sobrecarga de memoria del 5.69% 

En resumen:

## Turtles (2010)
- Autores: Ben-Ami et al., IBM Research y Technion.
- Hipervisor: KVM
- Paper: Turtles: Practical Nested Virtualization (ACM ASPLOS 2010)
- Aporte clave:
Primer dise√±o funcional y operativo de nested virtualization sobre Linux/KVM, permitiendo ejecutar un hipervisor completo (L1) sobre otro (L0) con rendimiento razonable.
- Impacto:
Fue la primera implementaci√≥n seria y completa de nested virtualization sobre x86. Introdujo el concepto t√©cnico que luego adoptar√≠an otros sistemas.
- Reconocimiento:
Publicado en una conferencia de alto nivel (ASPLOS), con fuerte impacto en la comunidad acad√©mica y kernel de Linux.

## NestCloud (2011)
- Autores: Tsinghua University + Intel Asia Pacific
- Hipervisor: Intel VT-x + arquitectura propia
- Paper: NestCloud: Towards Practical Nested Virtualization
- Aporte clave:
Propuso optimizaciones concretas (Guest Page Fault Bypassing, Virtual EPT, PV VMCS) que mejoraron sustancialmente el rendimiento de la virtualizaci√≥n anidada en escenarios de nube.
- Impacto:
No fue pionero en implementar nested virtualization, pero s√≠ mejor√≥ su viabilidad pr√°ctica, sobre todo para nubes privadas/publicas que requer√≠an overhead bajo.
- Reconocimiento:
T√©cnica s√≥lida, pero con menos impacto general que Turtles. No apareci√≥ en una conferencia tan influyente como ASPLOS.

## Entonces, recapitulando

| Tiempo | Tecnolog√≠a | Capacidad |
| --- | --- | --- |
| 1960s-90s | Hipervisores (IBM, VMware) | Crear VMs |
| 2000s | Clouds + Hypervisores modernos | Infraestructura virtualizada |
| 2010 | Turtles | Primera implementaci√≥n funcional de nested virtualization |
| 2011 | NestCloud | Optimizaci√≥n y viabilidad productiva en cloud |


> üìå Los hipervisores resolvieron la necesidad de compartir hardware. Nested virtualization resuelve la necesidad de virtualizar infraestructura completa dentro de infraestructura ya virtualizada.
 
Perfecto. Ya tenemos idea de c√≥mo funcionan las VM y virtualizaci√≥n anidada. Windows desarroll√≥ su alternativa comercial en Windows Server 2008: Hyper-V. Tenemos una comparativa t√©cnica entre el hipervisor de windows y linux:

| Hypervisor   | Hyper-V (Windows) | QEMU + KVM (Linux) |
| --------- | -------- | ------ |
| Virtualizador | QEMU | No usa uno externo: es nativo |
| Interfaz Host-Guest | VirtIO | VMBus + VSC/VSP |
| CPU Extensions | VT-x/AMD-V | VT-x/AMD-V |
| Aceleraci√≥n por hardware | S√≠ (KVM) | S√≠ (nativa) |
| Contenedores | Docker/LXC con namespaces | Windows Containers, WSL2 |

¬øQu√© sucedio entonces?

Vmware es un hipervisor de tipo 2 en el cual instalamos un SO que ejecuta un hipervisor de tipo 1. El problema aqu√≠ es que hyper-v, el hipervisor de tipo 1 de windows se apodera del hardware del so, lo que no permite funcionar.

Hyper-V Gen 2 permite UEFI y soporte para virtualizaci√≥n anidada solo si el host tambi√©n es Hyper-V en Windows 10/11 Pro, Enterprise o Server y si habilitamos expl√≠citamente la opci√≥n 

```powershell
ExposeVirtualizationExtensions
```

Esto permite que el sistema invitado detecte las extensiones VT-x/AMD-V necesarias para KVM, aunque no con el mismo rendimiento que en bare-metal.

Proxmox puede correr dentro de Hyper-V porque el kernel Linux detecta que se est√°n exponiendo las extensiones de virtualizaci√≥n (si est√°n habilitadas en la VM de Hyper-V).

Dentro de esa VM, Proxmox corre como si estuviera en una m√°quina f√≠sica, y puede levantar contenedores LXC o VMs con QEMU/KVM.

Sin embargo, el rendimiento de KVM anidado en Hyper-V es mucho menor que en KVM sobre KVM o sobre bare metal.

VMware Workstation tambi√©n permite virtualizaci√≥n anidada, pero:

- No siempre expone correctamente las extensiones de hardware.
- En ciertos casos, Linux no logra inicializar completamente KVM si las extensiones est√°n mal emuladas.
- Hyper-V, al estar m√°s cerca del hypervisor tipo 1, tiene mejor integraci√≥n con el kernel de Windows y puede pasar mejor las extensiones al kernel Linux invitado.

El kernel de Linux detecta si hay soporte para virtualizaci√≥n de hardware leyendo los registros CPUID, que pueden estar presentes si Hyper-V los expone.

Si el kernel no ve las extensiones, entonces kvm no puede cargarse y Proxmox no podr√° virtualizar nada dentro de esa VM.

# ¬øWSL2 permite virtualizacion anidada?

WSL2 corre sobre una VM ligera de Hyper-V, y esa VM de WSL2 no tiene acceso directo a las extensiones de virtualizaci√≥n (VT-x/AMD-V) por defecto. Pero:

Si est√°s en Windows 11 (o Windows 10 Pro 21H2+), puedes habilitar virtualizaci√≥n anidada en tu m√°quina host (Windows), y luego levantar una VM completa dentro de WSL2 usando QEMU con ciertas flags especiales (-enable-kvm).

Esto es experimental y no tan estable como KVM en Linux bare metal o en una VM Linux sobre KVM.

Por lo tanto, no puedo correr Proxmox dentro de WSL2 directamente.

WSL2 no es un sistema completo con soporte para ```systemd``` (a menos que lo actives manualmente), ni puede exponer interfaces como ```/dev/kvm``` con facilidad. Pero s√≠ puedes correr QEMU dentro de WSL2 con:

```--accel tcg``` (sin KVM): muy lento.

```--accel kvm``` (con acceso a ```/dev/kvm```): solo funciona si el host Windows expone virtualizaci√≥n anidada y la VM base de WSL2 la puede usar, lo cual es raro y poco fiable.

Entonces nos quedan tres alternativas.

- Instala Proxmox directamente en bare-metal.
- Usa Hyper-V Gen 2 con virtualizaci√≥n anidada activada para correr Proxmox como VM utilizando ```Set-VMProcessor -VMName "VMName" -ExposeVirtualizationExtensions $true```
- Instala Ubuntu o NixOS en dualboot o en una VM con virtualizaci√≥n anidada activada, y ah√≠ corres Proxmox o QEMU como se debe.

Se ver√≠a algo asi 

```plaintext
Intel CPU con VT-x
  ‚îî‚îÄ‚îÄ Hyper-V Hypervisor (tipo 1)
      ‚îî‚îÄ‚îÄ Hyper-V Gen 2
          ‚îú‚îÄ‚îÄ Kernel Linux con /dev/kvm
          ‚îî‚îÄ‚îÄ Proxmox VE instalado
              ‚îî‚îÄ‚îÄ VM Fedora 39
                  ‚îî‚îÄ‚îÄ Kubernetes corriendo
                      ‚îî‚îÄ‚îÄ Docker container
                          ‚îî‚îÄ‚îÄ App Java con Quarkus y GraalVM

```
Cada capa ve s√≥lo parte del sistema real, y se apoya en el subsuelo: hardware ‚Üí hipervisor ‚Üí kernel ‚Üí namespaces ‚Üí runtime ‚Üí contenedor.
- Si /dev/kvm no existe, no hay aceleraci√≥n de hardware.
- Si tu contenedor necesita /dev/kvm (como Firecracker o Kata Containers), no funcionar√° sin soporte del host.
- Si Docker corre sobre WSL2 y no hay VT-x expuesto, no puedes usar KVM dentro del contenedor.

# Mecanismos de virtualizaci√≥n

CPUID y MSR (Model-Specific Registers)

Cuando un hipervisor expone extensiones de virtualizaci√≥n (como VT-x), lo hace manipulando las respuestas al opcode CPUID, y permitiendo accesos a ciertos MSRs (Model-Specific Registers) como IA32_FEATURE_CONTROL. Si estas no son correctamente expuestas por el L0, el L1 no puede inicializar su hipervisor (L2).

EPT y shadow paging

Extended Page Tables (EPT) es una t√©cnica de Intel VT-x que permite una segunda capa de traducci√≥n de direcciones virtuales a f√≠sicas sin intervenci√≥n constante del hipervisor. Anteriormente, esto requer√≠a shadow page tables, que induc√≠an muchos VM Exits. NestCloud explota EPT para mantener la eficiencia.

VMBus vs VirtIO

VMBus (de Microsoft) y VirtIO (de Red Hat) son buses de paravirtualizaci√≥n: el primero est√° acoplado a los VSP/VSC (Virtualization Service Provider/Client), mientras que VirtIO es un est√°ndar abierto con dispositivos como virtio-blk, virtio-net, etc. La performance de I/O y el overhead dependen de c√≥mo el host exponga estos dispositivos virtualizados.

| Hypervisor | KVM (m√≥dulo de kernel) | Hyper-V Hypervisor (propietario) |
| --- | --- | --- |
| Virtualizador | QEMU | No usa uno externo: es nativo |
| Interfaz Host-Guest | VirtIO | VMBus + VSC/VSP |
| CPU Extensions | VT-x/AMD-V | VT-x/AMD-V |
| Aceleraci√≥n por hardware | S√≠ (KVM) | S√≠ (nativa) |
| Contenedores | Docker/LXC con namespaces | Windows Containers, WSL2 |

_Fernando Mart√≠nez ‚Äì Infraestructura, virtualizaci√≥n y sistemas distribuidos_